#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
LaborWatch â€” ë…¸ë™Â·ë²•ë ¹Â·ê¸ˆìœµÂ·ESG ì•Œë¦¼ë´‡ (Cloudtype + Telegram)
- ì†ŒìŠ¤: ë²•ì œì²˜ RSS, ê³ ìš©ë…¸ë™ë¶€ RSS, ê¸ˆìœµìœ„ RSS, DART(ê¸ˆê°ì›) ìµœì‹ ê³µì‹œ(HTML íŒŒì‹±), Google News RSS(í‚¤ì›Œë“œ)
- ê¸°ëŠ¥: ìˆ˜ì§‘ â†’ ì¹´í…Œê³ ë¦¬ ë¼ë²¨ â†’ í‚¤ì›Œë“œ í•„í„° â†’ ì¤‘ë³µì œê±°(SQLite) â†’ í…”ë ˆê·¸ë¨ ì „ì†¡
- ì‹¤í–‰: RUN_MODE=DAILY (1íšŒ ì‹¤í–‰) / RUN_MODE=POLL (ì£¼ê¸° ì‹¤í–‰)
- ENV:
    TELEGRAM_TOKEN, TELEGRAM_CHAT_ID
    RUN_MODE=DAILY|POLL (ê¸°ë³¸ DAILY)
    POLL_INTERVAL_S=900 (ê¸°ë³¸ 900ì´ˆ)
"""

import os, re, time, hashlib, sqlite3, textwrap, html
from datetime import datetime, timedelta, timezone
from typing import List, Dict
import requests
import xml.etree.ElementTree as ET

# ì„ íƒ ì˜ì¡´ì„±(HTML íŒŒì‹± ì•ˆì •ì„± â†‘)
try:
    from bs4 import BeautifulSoup  # type: ignore
    HAS_BS4 = True
except Exception:
    HAS_BS4 = False

from feeds_config import FEEDS   # <<< ë‹¹ì‹ ì´ ì˜¬ë¦´ feeds_config.py ì—ì„œ ì½ìŠµë‹ˆë‹¤.

# ===== ê¸°ë³¸ ì„¤ì • =====
KST = timezone(timedelta(hours=9))
DB_PATH = os.getenv("DB_PATH", "laborwatch.sqlite3")
TELEGRAM_TOKEN   = os.getenv("TELEGRAM_TOKEN", "")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", os.getenv("TELEGRAM_ID", ""))
RUN_MODE         = os.getenv("RUN_MODE", "DAILY").upper()
POLL_INTERVAL_S  = int(os.getenv("POLL_INTERVAL_S", "900"))

HEADERS = {"User-Agent": "LaborWatchBot/1.0 (+Cloudtype)"}

# ===== í‚¤ì›Œë“œ ì„¸íŠ¸(ë‰´ìŠ¤ í•„í„°ìš©) =====
KW = {
    "ë…¸ë™ë‰´ìŠ¤": r"(ë…¸ë™|ê·¼ë¡œ|ê·¼ë¡œê¸°ì¤€ë²•|ì‚°ì—…ì•ˆì „ë³´ê±´|ìµœì €ì„ê¸ˆ|ì£¼\s?52ì‹œê°„|ëª¨ì„±ë³´í˜¸|ìœ¡ì•„|ë‚¨ë…€ê³ ìš©í‰ë“±|ë…¸ì‚¬ê´€ê³„|í†µìƒì„ê¸ˆ|ì—°ì°¨|í¬ê´„ì„ê¸ˆ|ê·¼ë¡œì‹œê°„ë‹¨ì¶•|íƒ€ì„ì˜¤í”„)",
    "ê¸ˆìœµìœ„ë‰´ìŠ¤": r"(ê¸ˆìœµìœ„ì›íšŒ|ê¸ˆìœµìœ„|ì¦ì„ ìœ„|FIU|ì •ì±…ê¸ˆìœµ)",
    "ê¸ˆê°ì›ë‰´ìŠ¤": r"(ê¸ˆìœµê°ë…ì›|ê¸ˆê°ì›|DART|ì „ìê³µì‹œ)",
    "ESGë‰´ìŠ¤": r"\b(ESG|ì§€ì†ê°€ëŠ¥ê²½ì˜|ì§€ë°°êµ¬ì¡°|ESGê³µì‹œ|KCGS|í•œêµ­ESGê¸°ì¤€ì›)\b",
}

# ===== í…”ë ˆê·¸ë¨ =====
def tg_send(text: str):
    if not TELEGRAM_TOKEN or not TELEGRAM_CHAT_ID:
        return
    try:
        requests.post(
            f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage",
            json={"chat_id": TELEGRAM_CHAT_ID, "text": text, "disable_web_page_preview": True},
            timeout=15,
        )
    except Exception:
        pass

# ===== ì €ì¥ì†Œ =====
def ensure_db():
    with sqlite3.connect(DB_PATH) as c:
        c.execute("""
            CREATE TABLE IF NOT EXISTS seen (
              id TEXT PRIMARY KEY,
              title TEXT, link TEXT, pubdate TEXT, feed TEXT, cat TEXT, first_seen_ts TEXT
            )
        """)

def mk_id(title: str, link: str) -> str:
    base = (title or "") + "|" + (link or "")
    return hashlib.sha256(base.encode("utf-8")).hexdigest()

# ===== ê³µí†µ ë„ìš°ë¯¸ =====
def summarize(text: str, summary: str) -> str:
    t = (summary or "").strip()
    t = html.unescape(re.sub(r"<.*?>", "", t)).replace("&nbsp;", " ").strip()
    if not t:
        t = (text or "").strip()
    t = re.split(r"[ã€‚.!?]\s|[\n]", t)[0]
    return textwrap.shorten(t, width=180, placeholder="â€¦")

def parse_rss(xml_text: str) -> List[Dict]:
    root = ET.fromstring(xml_text)
    ns = {"atom": "http://www.w3.org/2005/Atom"}
    items = []

    # RSS 2.0
    for it in root.findall(".//item"):
        title = (it.findtext("title") or "").strip()
        link  = (it.findtext("link")  or "").strip()
        desc  = (it.findtext("description") or "").strip()
        pub   = (it.findtext("pubDate") or "").strip()
        items.append({"title": title, "link": link, "summary": desc, "pub": pub})

    # Atom
    for e in root.findall(".//atom:entry", ns):
        title = (e.findtext("atom:title", default="", namespaces=ns) or "").strip()
        link_el = e.find("atom:link", ns)
        link = (link_el.get("href") if link_el is not None else "").strip()
        summary = (e.findtext("atom:summary", default="", namespaces=ns) or
                   e.findtext("atom:content", default="", namespaces=ns) or "").strip()
        pub = (e.findtext("atom:updated", default="", namespaces=ns) or
               e.findtext("atom:published", default="", namespaces=ns) or "").strip()
        items.append({"title": title, "link": link, "summary": summary, "pub": pub})
    return items

def fetch_text(url: str) -> str:
    r = requests.get(url, headers=HEADERS, timeout=20)
    r.raise_for_status()
    r.encoding = r.apparent_encoding or r.encoding
    return r.text

# ===== ì¹´í…Œê³ ë¦¬ íŒë³„ =====
def categorize(feed_url: str, title: str, summary: str) -> str:
    t = f"{title} {summary}".strip()

    # ì¶œì²˜ ê¸°ë°˜ ìš°ì„ 
    if "fsc.go.kr" in feed_url:
        return "ê¸ˆìœµìœ„ ë³´ë„ìë£Œ"
    if "moel.go.kr" in feed_url and "lawinfo" in feed_url:
        return "ì…ë²•Â·í–‰ì •ì˜ˆê³ "
    if "moel.go.kr" in feed_url:
        return "ë…¸ë™ë¶€ ì†Œì‹"
    if "moleg.go.kr" in feed_url and "ll_rss" in feed_url:
        return "ìµœì‹  ì‹œí–‰ë²•ë ¹"
    if "moleg.go.kr" in feed_url and "li_rss" in feed_url:
        return "ì…ë²•ì˜ˆê³ "
    if "news.google.com" in feed_url:
        # í‚¤ì›Œë“œ ìš°ì„  ë¼ë²¨ë§
        for label, pat in KW.items():
            if re.search(pat, t, re.I): return label
        return "ë‰´ìŠ¤"

    # í‚¤ì›Œë“œë¡œ ë°±ì—… ë¼ë²¨ë§
    for label, pat in KW.items():
        if re.search(pat, t, re.I): return label
    return "ê¸°íƒ€"

# ===== íŠ¹ìˆ˜ ì†ŒìŠ¤: DART ìµœì‹  ê³µì‹œ(ê¸ˆê°ì›) =====
def collect_from_dart(url: str) -> List[Dict]:
    """
    ê¸ˆê°ì› DART: ê³µì‹ RSSê°€ ì•„ë‹Œ ê²½ìš°ê°€ ìˆìœ¼ë¯€ë¡œ ë©”ì¸/ëª©ë¡ HTMLì„ íŒŒì‹±í•œë‹¤.
    - https://dart.fss.or.kr/dsac001/main.do  (ìµœê·¼ê³µì‹œ ì˜ì—­)
    HTML êµ¬ì¡°ê°€ ë°”ë€Œë”ë¼ë„ 'rcpNo' ë˜ëŠ” '/dsaf001/main.do?rcpNo=' í˜•íƒœì˜ ë§í¬ë¥¼ ì£¼ë¡œ ì°¾ëŠ”ë‹¤.
    """
    out = []
    try:
        html_text = fetch_text(url)
    except Exception:
        return out

    if not HAS_BS4:
        # BeautifulSoupì´ ì—†ìœ¼ë©´ ì •ê·œì‹ìœ¼ë¡œ ìµœì†Œ ì •ë³´ë§Œ íŒŒì‹±
        for m in re.finditer(r'href="(/dsaf001/main\.do\?rcpNo=\d+)[^"]*".*?>([^<]+)</a>', html_text):
            link = "https://dart.fss.or.kr" + m.group(1)
            title = html.unescape(m.group(2)).strip()
            out.append({"title": title, "link": link, "summary": "", "pub": ""})
        return out

    soup = BeautifulSoup(html_text, "html.parser")
    # ìµœê·¼ê³µì‹œ í…Œì´ë¸” ì˜ì—­ì—ì„œ ë§í¬/ì œëª©/ì¼ì ì¶”ì¶œ (ìœ ì—°í•œ ì„ íƒì)
    for a in soup.select('a[href*="/dsaf001/main.do?rcpNo="]'):
        title = a.get_text(strip=True)
        href = a.get("href") or ""
        link = "https://dart.fss.or.kr" + href
        # ê°€ëŠ¥í•˜ë©´ ê°™ì€ í–‰ì˜ ë‚ ì§œë„ ì¶”ì¶œ
        pub = ""
        tr = a.find_parent("tr")
        if tr:
            tds = [td.get_text(" ", strip=True) for td in tr.find_all("td")]
            # ë³´í†µ [ë²ˆí˜¸, ê³µì‹œë²ˆí˜¸, íšŒì‚¬ëª…, ë³´ê³ ì„œëª…, ì ‘ìˆ˜ì¼ì, ...] í˜•íƒœ
            for token in tds[::-1]:
                if re.search(r"\d{4}-\d{2}-\d{2}", token):
                    pub = token
                    break
        out.append({"title": title, "link": link, "summary": "", "pub": pub})
    return out

# ===== ë©”ì¸ ìˆ˜ì§‘ =====
def collect_items() -> List[Dict]:
    items: List[Dict] = []

    for cat, urls in FEEDS.items():
        for u in urls:
            try:
                if "dart.fss.or.kr" in u:
                    items.extend(collect_from_dart(u))
                elif u.startswith("http"):
                    # RSS/Atom ì‹œë„
                    txt = fetch_text(u)
                    # RSS íŒŒì‹± ì‹¤íŒ¨ ì‹œ HTMLì¼ ìˆ˜ë„ ìˆìœ¼ë‹ˆ Google News ë“±ì€ ê·¸ëŒ€ë¡œ RSSë¡œ íŒŒì‹± ê°€ëŠ¥
                    try:
                        items_from_rss = parse_rss(txt)
                        for it in items_from_rss:
                            it["feed"] = u
                            items.append(it)
                    except Exception:
                        # RSSê°€ ì•„ë‹ˆë©´(HTML) ë‹¨ìˆœ ë§í¬ ìŠ¤í‚´ìœ¼ë¡œ ìŠ¤í‚µ
                        # (ì •ì±…ë¸Œë¦¬í•‘ HTML ë“±ì€ í•„ìš” ì‹œ ë³„ë„ íŒŒì„œ ì¶”ê°€)
                        pass
                else:
                    continue
            except Exception as e:
                items.append({"title": f"[ERR] {u}", "link": "", "summary": str(e), "pub": "", "feed": u})

    # FEEDSì˜ ì¹´í…Œê³ ë¦¬ëŠ” 'ì¶œì²˜ ê·¸ë£¹'ì´ê³ , ì‹¤ì œ ë°œì†¡ìš© ë¼ë²¨ì€ categorize()ë¡œ ìµœì¢… ê²°ì •
    for it in items:
        it.setdefault("feed", "unknown")
        it["cat"] = categorize(it["feed"], it.get("title",""), it.get("summary",""))

    return items

# ===== ì²˜ë¦¬ & ì „ì†¡ =====
def render_msg(it: Dict) -> str:
    title = (it.get("title") or "").strip()
    link  = (it.get("link")  or "").strip()
    summ  = summarize(title, it.get("summary") or "")
    pub   = (it.get("pub")   or "").strip()
    feed  = it.get("feed") or ""
    cat   = it.get("cat")  or "ì •ë³´"
    return (
        f"ğŸ”” {cat}\n"
        f"â€¢ ì œëª©: {title}\n"
        f"â€¢ ìš”ì•½: {summ}\n"
        f"â€¢ ë‚ ì§œ: {pub}\n"
        f"â€¢ ì¶œì²˜: {feed}\n"
        f"{link}"
    )

def process_once() -> int:
    ensure_db()
    now = datetime.now(KST)
    items = collect_items()
    sent = 0

    with sqlite3.connect(DB_PATH) as c:
        for it in items:
            uid = mk_id(it.get("title",""), it.get("link",""))
            if not uid: 
                continue
            cur = c.execute("SELECT 1 FROM seen WHERE id=?", (uid,)).fetchone()
            if cur:
                continue
            # í•„í„°: ë‰´ìŠ¤ ë¼ë²¨ì€ í‚¤ì›Œë“œê°€ ì‹¤ì œë¡œ ë“¤ì–´ìˆëŠ”ì§€ë§Œ í•œë²ˆ ë” ì²´í¬(ì˜¤ê²€ì¶œ ë°©ì§€)
            if it["cat"] in ("ë…¸ë™ë‰´ìŠ¤","ê¸ˆìœµìœ„ë‰´ìŠ¤","ê¸ˆê°ì›ë‰´ìŠ¤","ESGë‰´ìŠ¤"):
                text_for_match = f"{it.get('title','')} {it.get('summary','')}"
                pat = KW[it["cat"]]
                if not re.search(pat, text_for_match, re.I):
                    continue

            c.execute(
                "INSERT INTO seen (id,title,link,pubdate,feed,cat,first_seen_ts) VALUES (?,?,?,?,?,?,?)",
                (uid, it.get("title",""), it.get("link",""), it.get("pub",""), it.get("feed",""), it.get("cat",""), now.isoformat())
            )
            tg_send(render_msg(it))
            time.sleep(0.4)
            sent += 1

    if sent == 0:
        tg_send(f"âœ… {now.strftime('%Y-%m-%d')} í˜„ì¬ ì‹ ê·œ ì•Œë¦¼ ì—†ìŒ")
    return sent

def run_daily():
    process_once()

def run_poll():
    # ì‹œì‘ ì¦‰ì‹œ 1íšŒ, ì´í›„ ì£¼ê¸°
    while True:
        try:
            process_once()
        except Exception as e:
            tg_send(f"[LaborWatch ì˜¤ë¥˜] {e}")
        time.sleep(POLL_INTERVAL_S)

if __name__ == "__main__":
    if RUN_MODE == "POLL":
        run_poll()
    else:
        run_daily()
