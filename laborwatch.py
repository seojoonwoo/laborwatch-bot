#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
LaborWatch â€” ë…¸ë™Â·ë²•ë ¹Â·ê¸ˆìœµÂ·ESG ì•Œë¦¼ë´‡ (Cloudtype + Telegram)

ìš”ì²­ ë°˜ì˜:
1) ë²•ë ¹(ì…ë²•ì˜ˆê³ /ì‹œí–‰) ì•Œë¦¼ì€ "ë…¸ë™ê´€ê³„ ë²•ë ¹"ë§Œ í•„í„°ë§(í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸).
2) í•œêµ­ESGê¸°ì¤€ì› ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ëŠ” Top3(ë‹¤ë§¤ì²´ ì¤‘ë³µ ë¹ˆë„ ê·¼ì‚¬ì¹˜)ë§Œ ì „ì†¡.
3) ESG ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ë„ Top3ë§Œ ì „ì†¡.
4) ê¸ˆìœµìœ„ ë³´ë„ìë£Œ, ë…¸ë™ë¶€ ì†Œì‹ ë“±ì€ ê¸°ì¡´ëŒ€ë¡œ.

ì‹¤í–‰ ëª¨ë“œ:
- RUN_MODE=DAILY  : 1íšŒ ì‹¤í–‰(Cloudtype Cron: 0 8 * * * python laborwatch.py)
- RUN_MODE=POLL   : ì£¼ê¸° ì‹¤í–‰(í™˜ê²½ë³€ìˆ˜ POLL_INTERVAL_S ì´ˆ)

ENV:
  TELEGRAM_TOKEN, TELEGRAM_CHAT_ID
  RUN_MODE=DAILY|POLL (ê¸°ë³¸ DAILY)
  POLL_INTERVAL_S=900
"""

import os, re, time, hashlib, sqlite3, textwrap, html
from datetime import datetime, timedelta, timezone
from typing import List, Dict
import requests
import xml.etree.ElementTree as ET

# ì„ íƒ ì˜ì¡´ì„±(HTML íŒŒì‹± ì•ˆì •ì„± â†‘)
try:
    from bs4 import BeautifulSoup  # type: ignore
    HAS_BS4 = True
except Exception:
    HAS_BS4 = False

from feeds_config import FEEDS

# ===== ê¸°ë³¸ ì„¤ì • =====
KST = timezone(timedelta(hours=9))
DB_PATH = os.getenv("DB_PATH", "laborwatch.sqlite3")
TELEGRAM_TOKEN   = os.getenv("TELEGRAM_TOKEN", "")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", os.getenv("TELEGRAM_ID", ""))
RUN_MODE         = os.getenv("RUN_MODE", "DAILY").upper()
POLL_INTERVAL_S  = int(os.getenv("POLL_INTERVAL_S", "900"))
HEADERS = {"User-Agent": "LaborWatchBot/1.0 (+Cloudtype)"}

# ===== í‚¤ì›Œë“œ ì„¸íŠ¸(ë‰´ìŠ¤ í•„í„°ìš©) =====
KW = {
    "ë…¸ë™ë‰´ìŠ¤": r"(ë…¸ë™|ê·¼ë¡œ|ê·¼ë¡œê¸°ì¤€ë²•|ì‚°ì—…ì•ˆì „ë³´ê±´|ìµœì €ì„ê¸ˆ|ì£¼\s?52ì‹œê°„|ëª¨ì„±ë³´í˜¸|ìœ¡ì•„|ë‚¨ë…€ê³ ìš©í‰ë“±|ë…¸ì‚¬ê´€ê³„|í†µìƒì„ê¸ˆ|ì—°ì°¨|í¬ê´„ì„ê¸ˆ|ê·¼ë¡œì‹œê°„ë‹¨ì¶•|íƒ€ì„ì˜¤í”„)",
    "ê¸ˆìœµìœ„ë‰´ìŠ¤": r"(ê¸ˆìœµìœ„ì›íšŒ|ê¸ˆìœµìœ„|ì¦ì„ ìœ„|FIU|ì •ì±…ê¸ˆìœµ)",
    "ê¸ˆê°ì›ë‰´ìŠ¤": r"(ê¸ˆìœµê°ë…ì›|ê¸ˆê°ì›|DART|ì „ìê³µì‹œ)",
    "ESGë‰´ìŠ¤": r"\b(ESG|ì§€ì†ê°€ëŠ¥ê²½ì˜|ì§€ë°°êµ¬ì¡°|ESGê³µì‹œ|KCGS|í•œêµ­ESGê¸°ì¤€ì›)\b",
}

# ===== ë…¸ë™ê´€ê³„ ë²•ë ¹ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ =====
LABOR_LAWS = [
    "ê·¼ë¡œê¸°ì¤€ë²•", "ì‚°ì—…ì•ˆì „ë³´ê±´ë²•", "ìµœì €ì„ê¸ˆë²•", "ë‚¨ë…€ê³ ìš©í‰ë“±", "ê³ ìš©ë³´í—˜ë²•",
    "ê·¼ë¡œìí‡´ì§ê¸‰ì—¬", "ê¸°ê°„ì œ ë° ë‹¨ì‹œê°„ê·¼ë¡œì ë³´í˜¸", "íŒŒê²¬ê·¼ë¡œìë³´í˜¸",
    "ë…¸ë™ì¡°í•© ë° ë…¸ë™ê´€ê³„ì¡°ì •ë²•", "ê·¼ë¡œë³µì§€ê¸°ë³¸ë²•", "ê³ ìš©ì •ì±… ê¸°ë³¸ë²•",
    "ì§ì—…ì•ˆì •ë²•", "ì‚°ì¬ë³´í—˜", "ëª¨ì„±ë³´í˜¸", "ìœ¡ì•„ê¸° ê·¼ë¡œì‹œê°„ ë‹¨ì¶•", "ë‚¨ë…€ê³ ìš©í‰ë“±ë²•",
]
LABOR_LAW_PAT = re.compile("|".join(LABOR_LAWS), re.I)
KCGS_PAT = re.compile(r"(í•œêµ­ESGê¸°ì¤€ì›|KCGS)", re.I)

# ===== í…”ë ˆê·¸ë¨ =====
def tg_send(text: str):
    if not TELEGRAM_TOKEN or not TELEGRAM_CHAT_ID:
        return
    try:
        requests.post(
            f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage",
            json={"chat_id": TELEGRAM_CHAT_ID, "text": text, "disable_web_page_preview": True},
            timeout=15,
        )
    except Exception:
        pass

# ===== ì €ì¥ì†Œ =====
def ensure_db():
    with sqlite3.connect(DB_PATH) as c:
        c.execute("""
            CREATE TABLE IF NOT EXISTS seen (
              id TEXT PRIMARY KEY,
              title TEXT, link TEXT, pubdate TEXT, feed TEXT, cat TEXT, first_seen_ts TEXT
            )
        """)

def mk_id(title: str, link: str) -> str:
    base = (title or "") + "|" + (link or "")
    return hashlib.sha256(base.encode("utf-8")).hexdigest()

# ===== ê³µí†µ ë„ìš°ë¯¸ =====
def summarize(title: str, summary: str) -> str:
    t = (summary or "").strip()
    t = html.unescape(re.sub(r"<.*?>", "", t)).replace("&nbsp;", " ").strip()
    if not t:
        t = (title or "").strip()
    t = re.split(r"[ã€‚.!?]\s|[\n]", t)[0]
    return textwrap.shorten(t, width=180, placeholder="â€¦")

def parse_rss(xml_text: str) -> List[Dict]:
    root = ET.fromstring(xml_text)
    ns = {"atom": "http://www.w3.org/2005/Atom"}
    items = []

    # RSS 2.0
    for it in root.findall(".//item"):
        title = (it.findtext("title") or "").strip()
        link  = (it.findtext("link")  or "").strip()
        desc  = (it.findtext("description") or "").strip()
        pub   = (it.findtext("pubDate") or "").strip()
        items.append({"title": title, "link": link, "summary": desc, "pub": pub})

    # Atom
    for e in root.findall(".//atom:entry", ns):
        title = (e.findtext("atom:title", default="", namespaces=ns) or "").strip()
        link_el = e.find("atom:link", ns)
        link = (link_el.get("href") if link_el is not None else "").strip()
        summary = (e.findtext("atom:summary", default="", namespaces=ns) or
                   e.findtext("atom:content", default="", namespaces=ns) or "").strip()
        pub = (e.findtext("atom:updated", default="", namespaces=ns) or
               e.findtext("atom:published", default="", namespaces=ns) or "").strip()
        items.append({"title": title, "link": link, "summary": summary, "pub": pub})
    return items

def fetch_text(url: str) -> str:
    r = requests.get(url, headers=HEADERS, timeout=20)
    r.raise_for_status()
    r.encoding = r.apparent_encoding or r.encoding
    return r.text

# ===== ì¹´í…Œê³ ë¦¬ íŒë³„ =====
def categorize(feed_url: str, title: str, summary: str) -> str:
    t = f"{title} {summary}".strip()

    # ì¶œì²˜ ê¸°ë°˜ ìš°ì„ 
    if "fsc.go.kr" in feed_url:
        return "ê¸ˆìœµìœ„ ë³´ë„ìë£Œ"
    if "moel.go.kr" in feed_url and "lawinfo" in feed_url:
        return "ì…ë²•Â·í–‰ì •ì˜ˆê³ "
    if "moel.go.kr" in feed_url:
        return "ë…¸ë™ë¶€ ì†Œì‹"
    if "moleg.go.kr" in feed_url and "ll_rss" in feed_url:
        return "ìµœì‹  ì‹œí–‰ë²•ë ¹"
    if "moleg.go.kr" in feed_url and "li_rss" in feed_url:
        return "ì…ë²•ì˜ˆê³ "
    if "news.google.com" in feed_url:
        for label, pat in KW.items():
            if re.search(pat, t, re.I): return label
        return "ë‰´ìŠ¤"

    for label, pat in KW.items():
        if re.search(pat, t, re.I): return label
    return "ê¸°íƒ€"

# ===== íŠ¹ìˆ˜ ì†ŒìŠ¤: DART ìµœì‹  ê³µì‹œ(ê¸ˆê°ì›) =====
def collect_from_dart(url: str) -> List[Dict]:
    out = []
    try:
        html_text = fetch_text(url)
    except Exception:
        return out

    if not HAS_BS4:
        for m in re.finditer(r'href="(/dsaf001/main\.do\?rcpNo=\d+)[^"]*".*?>([^<]+)</a>', html_text):
            link = "https://dart.fss.or.kr" + m.group(1)
            title = html.unescape(m.group(2)).strip()
            out.append({"title": title, "link": link, "summary": "", "pub": ""})
        return out

    soup = BeautifulSoup(html_text, "html.parser")
    for a in soup.select('a[href*="/dsaf001/main.do?rcpNo="]'):
        title = a.get_text(strip=True)
        href = a.get("href") or ""
        link = "https://dart.fss.or.kr" + href
        pub = ""
        tr = a.find_parent("tr")
        if tr:
            tds = [td.get_text(" ", strip=True) for td in tr.find_all("td")]
            for token in tds[::-1]:
                if re.search(r"\d{4}-\d{2}-\d{2}", token):
                    pub = token
                    break
        out.append({"title": title, "link": link, "summary": "", "pub": pub})
    return out

# ===== ìˆ˜ì§‘ =====
def collect_items() -> List[Dict]:
    items: List[Dict] = []
    for _cat, urls in FEEDS.items():
        for u in urls:
            try:
                if "dart.fss.or.kr" in u:
                    items.extend(collect_from_dart(u))
                elif u.startswith("http"):
                    txt = fetch_text(u)
                    try:
                        for it in parse_rss(txt):
                            it["feed"] = u
                            items.append(it)
                    except Exception:
                        pass
            except Exception as e:
                items.append({"title": f"[ERR] {u}", "link": "", "summary": str(e), "pub": "", "feed": u})

    for it in items:
        it.setdefault("feed", "unknown")
        it["cat"] = categorize(it["feed"], it.get("title",""), it.get("summary",""))
    return items

# ===== ì¸ê¸°(ì¡°íšŒìˆ˜ ê·¼ì‚¬) Top3 ì„ íƒ =====
def normalize_title(t: str) -> str:
    t = re.sub(r"\s+", " ", t or "").strip().lower()
    t = re.sub(r"\[[^\]]+\]|\([^)]+\)", "", t)
    return t

def pick_top3_by_popularity(items: List[Dict]) -> List[Dict]:
    buckets: Dict[str, List[Dict]] = {}
    for it in items:
        key = normalize_title(it.get("title","")) or it.get("link","")
        buckets.setdefault(key, []).append(it)
    ranked = sorted(
        buckets.values(),
        key=lambda grp: (len(grp), max([(it.get("pub") or "") for it in grp])),
        reverse=True
    )
    top = []
    for grp in ranked:
        top.append(grp[0])
        if len(top) == 3:
            break
    return top

def is_labor_law_item(it: Dict) -> bool:
    """ì…ë²•ì˜ˆê³ /ì‹œí–‰/í–‰ì •ì˜ˆê³ ëŠ” 'ë…¸ë™ê´€ê³„ ë²•ë ¹'ë§Œ í†µê³¼"""
    if it.get("cat") in ("ì…ë²•ì˜ˆê³ ", "ìµœì‹  ì‹œí–‰ë²•ë ¹", "ì…ë²•Â·í–‰ì •ì˜ˆê³ "):
        text = f"{it.get('title','')} {it.get('summary','')}"
        return bool(LABOR_LAW_PAT.search(text))
    return True

# ===== ì²˜ë¦¬ & ì „ì†¡ =====
def render_msg(it: Dict) -> str:
    title = (it.get("title") or "").strip()
    link  = (it.get("link")  or "").strip()
    summ  = summarize(title, it.get("summary") or "")
    pub   = (it.get("pub")   or "").strip()
    feed  = it.get("feed") or ""
    cat   = it.get("cat")  or "ì •ë³´"
    return (
        f"ğŸ”” {cat}\n"
        f"â€¢ ì œëª©: {title}\n"
        f"â€¢ ìš”ì•½: {summ}\n"
        f"â€¢ ë‚ ì§œ: {pub}\n"
        f"â€¢ ì¶œì²˜: {feed}\n"
        f"{link}"
    )

def process_once() -> int:
    ensure_db()
    now = datetime.now(KST)
    items = collect_items()

    # --- KCGS/ESG ë‰´ìŠ¤ ë¶„ë¦¬ ---
    kcgs_items = [it for it in items if it.get("cat") in ("ESGë‰´ìŠ¤","ë‰´ìŠ¤")
                  and KCGS_PAT.search(f"{it.get('title','')} {it.get('summary','')}")]
    esg_items  = [it for it in items if it.get("cat") == "ESGë‰´ìŠ¤"
                  and not KCGS_PAT.search(f"{it.get('title','')} {it.get('summary','')}")]

    kcgs_top3 = pick_top3_by_popularity(kcgs_items)
    esg_top3  = pick_top3_by_popularity(esg_items)

    allowed_top_ids = { mk_id(it.get("title",""), it.get("link","")) for it in (kcgs_top3 + esg_top3) }

    sent = 0
    with sqlite3.connect(DB_PATH) as c:
        for it in items:
            # 1) ë…¸ë™ê´€ê³„ ë²•ë ¹ í•„í„°
            if not is_labor_law_item(it):
                continue

            # 2) ESG/í•œêµ­ESGê¸°ì¤€ì› ë‰´ìŠ¤ëŠ” Top3ë§Œ í—ˆìš©
            if it.get("cat") in ("ESGë‰´ìŠ¤","ë‰´ìŠ¤"):
                uid = mk_id(it.get("title",""), it.get("link",""))
                if uid not in allowed_top_ids:
                    continue

            # 3) ì¤‘ë³µ ì „ì†¡ ë°©ì§€
            uid = mk_id(it.get("title",""), it.get("link",""))
            if not uid:
                continue
            if c.execute("SELECT 1 FROM seen WHERE id=?", (uid,)).fetchone():
                continue

            c.execute(
                "INSERT INTO seen (id,title,link,pubdate,feed,cat,first_seen_ts) VALUES (?,?,?,?,?,?,?)",
                (uid, it.get("title",""), it.get("link",""), it.get("pub",""), it.get("feed",""), it.get("cat",""), now.isoformat())
            )
            tg_send(render_msg(it))
            time.sleep(0.35)
            sent += 1

    if sent == 0:
        tg_send(f"âœ… {now.strftime('%Y-%m-%d')} ì‹ ê·œ ì•Œë¦¼ ì—†ìŒ (í•„í„° ì ìš©)")
    return sent

def run_daily():
    process_once()

def run_poll():
    while True:
        try:
            process_once()
        except Exception as e:
            tg_send(f"[LaborWatch ì˜¤ë¥˜] {e}")
        time.sleep(POLL_INTERVAL_S)

if __name__ == "__main__":
    if RUN_MODE == "POLL":
        run_poll()
    else:
        run_daily()
