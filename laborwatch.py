#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
LaborWatch â€” ë…¸ë™Â·ë²•ë ¹Â·ê¸ˆìœµÂ·ESG ì•Œë¦¼ë´‡ (Cloudtype + Telegram)

ê¸°ëŠ¥ ìš”ì•½:
1) ì „ì†¡ ëŒ€ìƒì€ 'ì•Œë¦¼ ê¸°ì¤€ì¼ì˜ ì „ë‚ (00:00~23:59, KST)'ì— ìƒˆë¡œ ì˜¬ë¼ì˜¨ í•­ëª©ë§Œ.
2) ë²•ë ¹(ì…ë²•ì˜ˆê³ /ì‹œí–‰/í–‰ì •ì˜ˆê³ )ì€ 'ë…¸ë™ê´€ê³„ ë²•ë ¹'ë§Œ í•„í„°ë§(ë²•ëª… + ë°±ì—… í‚¤ì›Œë“œ).
3) ESG ë‰´ìŠ¤ëŠ” Top3, í•œêµ­ESGê¸°ì¤€ì› ê´€ë ¨ ë‰´ìŠ¤ë„ Top3(ë‹¤ë§¤ì²´ ì¤‘ë³µ ê·¼ì‚¬)ë§Œ ì „ì†¡.
4) ì˜¤ì „ 8ì‹œ ìë™ ì•Œë¦¼(Cloudtype ìŠ¤ì¼€ì¤„ UIê°€ ì—†ì–´ë„, ë‚´ë¶€ íƒ€ì´ë¨¸ë¡œ ì‹¤í–‰).
5) ì „ë‚  ì°½êµ¬ ìˆ˜ì§‘ ìš”ì•½(ì¹´í…Œê³ ë¦¬ë³„ ê±´ìˆ˜ + ERR ê±´ìˆ˜)ë„ í•¨ê»˜ ì „ì†¡.

ENV:
  TELEGRAM_TOKEN, TELEGRAM_CHAT_ID
  RUN_MODE=DAILY|POLL (ê¶Œì¥: DAILY)
  POLL_INTERVAL_S=900
"""

import os, re, time, hashlib, sqlite3, textwrap, html
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional
import requests
import xml.etree.ElementTree as ET

# ì„ íƒ ì˜ì¡´ì„±(HTML íŒŒì‹± ì•ˆì •ì„± â†‘)
try:
    from bs4 import BeautifulSoup  # type: ignore
    HAS_BS4 = True
except Exception:
    HAS_BS4 = False

from feeds_config import FEEDS

# ===== ê¸°ë³¸ ì„¤ì • =====
KST = timezone(timedelta(hours=9))
DB_PATH = os.getenv("DB_PATH", "laborwatch.sqlite3")
TELEGRAM_TOKEN   = os.getenv("TELEGRAM_TOKEN", "")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", os.getenv("TELEGRAM_ID", ""))
RUN_MODE         = os.getenv("RUN_MODE", "DAILY").upper()
POLL_INTERVAL_S  = int(os.getenv("POLL_INTERVAL_S", "900"))
HEADERS = {"User-Agent": "LaborWatchBot/1.2 (+Cloudtype)"}

# ===== í‚¤ì›Œë“œ ì„¸íŠ¸(ë‰´ìŠ¤ í•„í„°ìš©) =====
KW = {
    "ë…¸ë™ë‰´ìŠ¤": r"(ë…¸ë™|ê·¼ë¡œ|ê·¼ë¡œê¸°ì¤€ë²•|ì‚°ì—…ì•ˆì „ë³´ê±´|ìµœì €ì„ê¸ˆ|ì£¼\s?52ì‹œê°„|ëª¨ì„±ë³´í˜¸|ìœ¡ì•„|ë‚¨ë…€ê³ ìš©í‰ë“±|ë…¸ì‚¬ê´€ê³„|í†µìƒì„ê¸ˆ|ì—°ì°¨|í¬ê´„ì„ê¸ˆ|ê·¼ë¡œì‹œê°„ë‹¨ì¶•|íƒ€ì„ì˜¤í”„)",
    "ê¸ˆìœµìœ„ë‰´ìŠ¤": r"(ê¸ˆìœµìœ„ì›íšŒ|ê¸ˆìœµìœ„|ì¦ì„ ìœ„|FIU|ì •ì±…ê¸ˆìœµ)",
    "ê¸ˆê°ì›ë‰´ìŠ¤": r"(ê¸ˆìœµê°ë…ì›|ê¸ˆê°ì›|DART|ì „ìê³µì‹œ)",
    "ESGë‰´ìŠ¤": r"\b(ESG|ì§€ì†ê°€ëŠ¥ê²½ì˜|ì§€ë°°êµ¬ì¡°|ESGê³µì‹œ|KCGS|í•œêµ­ESGê¸°ì¤€ì›)\b",
}

# ===== ë…¸ë™ê´€ê³„ ë²•ë ¹ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ + ë°±ì—… í‚¤ì›Œë“œ =====
LABOR_LAWS = [
    "ê·¼ë¡œê¸°ì¤€ë²•", "ì‚°ì—…ì•ˆì „ë³´ê±´ë²•", "ìµœì €ì„ê¸ˆë²•", "ë‚¨ë…€ê³ ìš©í‰ë“±", "ê³ ìš©ë³´í—˜ë²•",
    "ê·¼ë¡œìí‡´ì§ê¸‰ì—¬", "ê¸°ê°„ì œ ë° ë‹¨ì‹œê°„ê·¼ë¡œì ë³´í˜¸", "íŒŒê²¬ê·¼ë¡œìë³´í˜¸",
    "ë…¸ë™ì¡°í•© ë° ë…¸ë™ê´€ê³„ì¡°ì •ë²•", "ê·¼ë¡œë³µì§€ê¸°ë³¸ë²•", "ê³ ìš©ì •ì±… ê¸°ë³¸ë²•",
    "ì§ì—…ì•ˆì •ë²•", "ì‚°ì¬ë³´í—˜", "ëª¨ì„±ë³´í˜¸", "ìœ¡ì•„ê¸° ê·¼ë¡œì‹œê°„ ë‹¨ì¶•", "ë‚¨ë…€ê³ ìš©í‰ë“±ë²•",
]
LABOR_LAW_PAT    = re.compile("|".join(LABOR_LAWS), re.I)
LABOR_BACKUP_PAT = re.compile(r"(ê·¼ë¡œ|ë…¸ë™|ëª¨ì„±ë³´í˜¸|ìœ¡ì•„|ë‚¨ë…€ê³ ìš©í‰ë“±|ìµœì €ì„ê¸ˆ|ì‚°ì—…ì•ˆì „|í‡´ì§ê¸‰ì—¬)", re.I)

KCGS_PAT = re.compile(r"(í•œêµ­ESGê¸°ì¤€ì›|KCGS)", re.I)

# ===== í…”ë ˆê·¸ë¨ =====
def tg_send(text: str):
    if not TELEGRAM_TOKEN or not TELEGRAM_CHAT_ID:
        return
    try:
        requests.post(
            f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage",
            json={"chat_id": TELEGRAM_CHAT_ID, "text": text, "disable_web_page_preview": True},
            timeout=15,
        )
    except Exception:
        pass

# ===== ì €ì¥ì†Œ =====
def ensure_db():
    with sqlite3.connect(DB_PATH) as c:
        c.execute("""
            CREATE TABLE IF NOT EXISTS seen (
              id TEXT PRIMARY KEY,
              title TEXT, link TEXT, pubdate TEXT, feed TEXT, cat TEXT, first_seen_ts TEXT
            )
        """)

def mk_id(title: str, link: str) -> str:
    base = (title or "") + "|" + (link or "")
    return hashlib.sha256(base.encode("utf-8")).hexdigest()

# ===== ê³µí†µ ë„ìš°ë¯¸ =====
def summarize(title: str, summary: str) -> str:
    t = (summary or "").strip()
    t = html.unescape(re.sub(r"<.*?>", "", t)).replace("&nbsp;", " ").strip()
    if not t:
        t = (title or "").strip()
    t = re.split(r"[ã€‚.!?]\s|[\n]", t)[0]
    return textwrap.shorten(t, width=180, placeholder="â€¦")

def parse_rss(xml_text: str) -> List[Dict]:
    root = ET.fromstring(xml_text)
    ns = {"atom": "http://www.w3.org/2005/Atom"}
    items: List[Dict] = []

    # RSS 2.0
    for it in root.findall(".//item"):
        title = (it.findtext("title") or "").strip()
        link  = (it.findtext("link")  or "").strip()
        desc  = (it.findtext("description") or "").strip()
        pub   = (it.findtext("pubDate") or "").strip()
        items.append({"title": title, "link": link, "summary": desc, "pub": pub})

    # Atom
    for e in root.findall(".//atom:entry", ns):
        title = (e.findtext("atom:title", default="", namespaces=ns) or "").strip()
        link_el = e.find("atom:link", ns)
        link = (link_el.get("href") if link_el is not None else "").strip()
        summary = (e.findtext("atom:summary", default="", namespaces=ns) or
                   e.findtext("atom:content", default="", namespaces=ns) or "").strip()
        pub = (e.findtext("atom:updated", default="", namespaces=ns) or
               e.findtext("atom:published", default="", namespaces=ns) or "").strip()
        items.append({"title": title, "link": link, "summary": summary, "pub": pub})
    return items

def fetch_text(url: str) -> str:
    r = requests.get(url, headers=HEADERS, timeout=20)
    r.raise_for_status()
    r.encoding = r.apparent_encoding or r.encoding
    return r.text

# ===== ë‚ ì§œ íŒŒì„œ =====
DATE_PATTERNS = [
    "%a, %d %b %Y %H:%M:%S %z",    # Tue, 11 Nov 2025 07:00:00 +0900
    "%a, %d %b %Y %H:%M:%S %Z",    # Tue, 11 Nov 2025 07:00:00 KST
    "%Y-%m-%dT%H:%M:%S%z",         # 2025-11-11T07:00:00+09:00
    "%Y-%m-%dT%H:%M:%S.%f%z",      # 2025-11-11T07:00:00.000+09:00
    "%Y-%m-%d %H:%M:%S%z",
    "%Y-%m-%d %H:%M",
    "%Y-%m-%d",
    "%Y.%m.%d %H:%M",
    "%Y.%m.%d",
]

def parse_dt(text: str) -> Optional[datetime]:
    t = (text or "").strip()
    if not t:
        return None

    # Z â†’ +0000 ë³´ì •
    if t.endswith("Z") and "T" in t:
        try:
            dt = datetime.strptime(t, "%Y-%m-%dT%H:%M:%SZ")
            return dt.replace(tzinfo=timezone.utc).astimezone(KST)
        except Exception:
            pass

    for pat in DATE_PATTERNS:
        try:
            dt = datetime.strptime(t, pat)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=KST)
            return dt.astimezone(KST)
        except Exception:
            continue

    # ìˆ«ìë§Œ ê°•ì œ ì¶”ì¶œ: YYYY.MM.DD ë˜ëŠ” YYYY-MM-DD (+ optional HH:MM)
    m = re.search(r"(\d{4})[.\-/](\d{1,2})[.\-/](\d{1,2})(?:\s+(\d{1,2}):(\d{2}))?", t)
    if m:
        try:
            hh = int(m.group(4) or 0)
            mm = int(m.group(5) or 0)
            dt = datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)), hh, mm, tzinfo=KST)
            return dt
        except Exception:
            return None

    return None

# ===== ì¹´í…Œê³ ë¦¬ íŒë³„ =====
def categorize(feed_url: str, title: str, summary: str) -> str:
    t = f"{title} {summary}".strip()

    if "fsc.go.kr" in feed_url:
        return "ê¸ˆìœµìœ„ ë³´ë„ìë£Œ"
    if "moel.go.kr" in feed_url and "lawinfo" in feed_url:
        return "ì…ë²•Â·í–‰ì •ì˜ˆê³ "
    if "moel.go.kr" in feed_url:
        return "ë…¸ë™ë¶€ ì†Œì‹"
    if "moleg.go.kr" in feed_url and "ll_rss" in feed_url:
        return "ìµœì‹  ì‹œí–‰ë²•ë ¹"
    if "moleg.go.kr" in feed_url and "li_rss" in feed_url:
        return "ì…ë²•ì˜ˆê³ "
    if "news.google.com" in feed_url:
        for label, pat in KW.items():
            if re.search(pat, t, re.I):
                return label
        return "ë‰´ìŠ¤"

    for label, pat in KW.items():
        if re.search(pat, t, re.I):
            return label
    return "ê¸°íƒ€"

# ===== íŠ¹ìˆ˜ ì†ŒìŠ¤: DART ìµœì‹  ê³µì‹œ(ê¸ˆê°ì›) =====
def collect_from_dart(url: str) -> List[Dict]:
    out: List[Dict] = []
    try:
        html_text = fetch_text(url)
    except Exception:
        return out

    if not HAS_BS4:
        for m in re.finditer(r'href="(/dsaf001/main\.do\?rcpNo=\d+)[^"]*".*?>([^<]+)</a>', html_text):
            link = "https://dart.fss.or.kr" + m.group(1)
            title = html.unescape(m.group(2)).strip()
            out.append({"title": title, "link": link, "summary": "", "pub": ""})
        return out

    soup = BeautifulSoup(html_text, "html.parser")
    for a in soup.select('a[href*="/dsaf001/main.do?rcpNo="]'):
        title = a.get_text(strip=True)
        href = a.get("href") or ""
        link = "https://dart.fss.or.kr" + href
        pub = ""
        tr = a.find_parent("tr")
        if tr:
            tds = [td.get_text(" ", strip=True) for td in tr.find_all("td")]
            for token in tds[::-1]:
                if re.search(r"\d{4}-\d{2}-\d{2}", token):
                    pub = token
                    break
        out.append({"title": title, "link": link, "summary": "", "pub": pub})
    return out

# ===== ìˆ˜ì§‘ =====
def collect_items() -> List[Dict]:
    items: List[Dict] = []
    for _cat, urls in FEEDS.items():
        for u in urls:
            try:
                if "dart.fss.or.kr" in u:
                    items.extend(collect_from_dart(u))
                elif u.startswith("http"):
                    txt = fetch_text(u)
                    try:
                        for it in parse_rss(txt):
                            it["feed"] = u
                            items.append(it)
                    except Exception:
                        pass
            except Exception as e:
                items.append({
                    "title": f"[ERR] {u}",
                    "link": "",
                    "summary": str(e),
                    "pub": "",
                    "feed": u,
                })

    for it in items:
        it.setdefault("feed", "unknown")
        it["cat"] = categorize(it["feed"], it.get("title",""), it.get("summary",""))
        it["_dt"] = parse_dt(it.get("pub","") or "")
    return items

# ===== ì¸ê¸°(ì¡°íšŒìˆ˜ ê·¼ì‚¬) Top3 ì„ íƒ =====
def normalize_title(t: str) -> str:
    t = re.sub(r"\s+", " ", t or "").strip().lower()
    t = re.sub(r"\[[^\]]+\]|\([^)]+\)", "", t)
    return t

def pick_top3_by_popularity(items: List[Dict]) -> List[Dict]:
    buckets: Dict[str, List[Dict]] = {}
    for it in items:
        key = normalize_title(it.get("title","")) or it.get("link","")
        buckets.setdefault(key, []).append(it)

    ranked = sorted(
        buckets.values(),
        key=lambda grp: (
            len(grp),
            max([(it.get("_dt") or datetime.min.replace(tzinfo=KST)) for it in grp])
        ),
        reverse=True
    )

    top: List[Dict] = []
    for grp in ranked:
        rep = sorted(
            grp,
            key=lambda x: x.get("_dt") or datetime.min.replace(tzinfo=KST),
            reverse=True
        )[0]
        top.append(rep)
        if len(top) == 3:
            break
    return top

# ===== ë…¸ë™ê´€ê³„ ë²•ë ¹ í•„í„° =====
def is_labor_law_item(it: Dict) -> bool:
    if it.get("cat") in ("ì…ë²•ì˜ˆê³ ", "ìµœì‹  ì‹œí–‰ë²•ë ¹", "ì…ë²•Â·í–‰ì •ì˜ˆê³ "):
        text = f"{it.get('title','')} {it.get('summary','')}"
        if LABOR_LAW_PAT.search(text):
            return True
        # ë²•ëª…ì€ ì—†ì§€ë§Œ ë…¸ë™ ê´€ë ¨ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì œëª©/ìš”ì•½
        return bool(LABOR_BACKUP_PAT.search(text))
    return True

# ===== ì²˜ë¦¬ & ì „ì†¡ =====
def render_msg(it: Dict, window_label: str) -> str:
    title = (it.get("title") or "").strip()
    link  = (it.get("link")  or "").strip()
    summ  = summarize(title, it.get("summary") or "")
    pub   = (it.get("pub")   or "").strip()
    feed  = it.get("feed") or ""
    cat   = it.get("cat")  or "ì •ë³´"
    return (
        f"ğŸ”” {cat} ({window_label})\n"
        f"â€¢ ì œëª©: {title}\n"
        f"â€¢ ìš”ì•½: {summ}\n"
        f"â€¢ ë‚ ì§œ: {pub}\n"
        f"â€¢ ì¶œì²˜: {feed}\n"
        f"{link}"
    )

def process_once() -> int:
    """
    ê¸°ì¤€ì‹œê°(now, KST)ì—ì„œ 'ì „ë‚  00:00~23:59'ì— í•´ë‹¹í•˜ëŠ” í•­ëª©ë§Œ ì „ì†¡
    """
    ensure_db()
    now = datetime.now(KST)

    y_start = (now.date() - timedelta(days=1))
    y_end   = (now.date() - timedelta(days=1))
    win_start_dt = datetime(y_start.year, y_start.month, y_start.day, 0, 0, 0, tzinfo=KST)
    win_end_dt   = datetime(y_end.year,   y_end.month,   y_end.day,   23, 59, 59, tzinfo=KST)
    window_label = f"{y_start.strftime('%Y-%m-%d')} ì—…ë°ì´íŠ¸"

    items = collect_items()

    # 1) ë‚ ì§œ ìœˆë„ìš°(ì „ë‚ ) í•„í„°
    items = [it for it in items if it.get("_dt") and win_start_dt <= it["_dt"] <= win_end_dt]

    # 2) ì „ë‚  ì°½êµ¬ ìˆ˜ì§‘ ìš”ì•½(ì¹´í…Œê³ ë¦¬/ERR) ë¨¼ì € ì „ì†¡
    summary: Dict[str, int] = {}
    for it in items:
        if it.get("title", "").startswith("[ERR]"):
            key = "ERR"
        else:
            key = it.get("cat", "ê¸°íƒ€")
        summary[key] = summary.get(key, 0) + 1

    count_line = " | ".join(f"{k}:{v}" for k, v in summary.items()) if summary else "ìˆ˜ì§‘ 0ê±´"
    tg_send(f"ğŸ“Š {window_label} ì°½êµ¬ ìˆ˜ì§‘ ìš”ì•½: {count_line}")

    # 3) ESG/KCGS ë‰´ìŠ¤ ë¶„ë¦¬ í›„ Top3ë§Œ í—ˆìš©
    kcgs_items = [
        it for it in items
        if it.get("cat") in ("ESGë‰´ìŠ¤", "ë‰´ìŠ¤")
        and KCGS_PAT.search(f"{it.get('title','')} {it.get('summary','')}")
    ]
    esg_items = [
        it for it in items
        if it.get("cat") == "ESGë‰´ìŠ¤"
        and not KCGS_PAT.search(f"{it.get('title','')} {it.get('summary','')}")
    ]

    kcgs_top3 = pick_top3_by_popularity(kcgs_items)
    esg_top3  = pick_top3_by_popularity(esg_items)
    allowed_top_ids = {mk_id(it.get("title",""), it.get("link","")) for it in (kcgs_top3 + esg_top3)}

    sent = 0
    with sqlite3.connect(DB_PATH) as c:
        for it in items:
            # 4) ë…¸ë™ê´€ê³„ ë²•ë ¹ í•„í„°
            if not is_labor_law_item(it):
                continue

            # 5) ESG/í•œêµ­ESGê¸°ì¤€ì› ë‰´ìŠ¤ëŠ” Top3ë§Œ í—ˆìš©
            if it.get("cat") in ("ESGë‰´ìŠ¤", "ë‰´ìŠ¤"):
                uid_tmp = mk_id(it.get("title",""), it.get("link",""))
                if uid_tmp not in allowed_top_ids:
                    continue

            # 6) ì¤‘ë³µ ì „ì†¡ ë°©ì§€
            uid = mk_id(it.get("title",""), it.get("link",""))
            if not uid:
                continue
            if c.execute("SELECT 1 FROM seen WHERE id=?", (uid,)).fetchone():
                continue

            c.execute(
                "INSERT INTO seen (id,title,link,pubdate,feed,cat,first_seen_ts) "
                "VALUES (?,?,?,?,?,?,?)",
                (
                    uid,
                    it.get("title",""),
                    it.get("link",""),
                    it.get("pub",""),
                    it.get("feed",""),
                    it.get("cat",""),
                    now.isoformat(),
                ),
            )
            tg_send(render_msg(it, window_label))
            time.sleep(0.35)
            sent += 1

    if sent == 0:
        tg_send(f"âœ… {window_label} ê¸°ì¤€ ì‹ ê·œ ì•Œë¦¼ ì—†ìŒ (ì „ë‚  í•„í„° ì ìš©)")
    return sent

def run_daily():
    process_once()

def run_poll():
    while True:
        try:
            process_once()
        except Exception as e:
            tg_send(f"[LaborWatch ì˜¤ë¥˜] {e}")
        time.sleep(POLL_INTERVAL_S)

if __name__ == "__main__":
    # ìŠ¤ì¼€ì¤„ UI ì—†ì´ë„ ë§¤ì¼ 08:00 ì‹¤í–‰ë˜ë„ë¡ ë‚´ë¶€ íƒ€ì´ë¨¸ ì œê³µ
    if RUN_MODE == "POLL":
        run_poll()
    elif RUN_MODE == "DAILY":
        while True:
            now = datetime.now(KST)
            if now.hour == 8 and now.minute == 0:
                try:
                    process_once()
                except Exception as e:
                    tg_send(f"[ì˜¤ë¥˜] {e}")
                time.sleep(60)  # 1ë¶„ ë™ì•ˆ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
            else:
                time.sleep(30)
    else:
        run_daily()
