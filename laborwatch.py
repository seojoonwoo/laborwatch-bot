#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
News â†’ Telegram í†µí•© ì•Œë¦¼ë´‡ (ì¸ì‚¬ë…¸ë¬´/ë…¸ë™ë²•/ê³ ìš©ë…¸ë™ë¶€/ê¸ˆìœµìœ„/KCGS ì „ìš©)

- ESG ì¼ë°˜, ê¸ˆìœµê°ë…ì›(FSS) ê´€ë ¨ ì•Œë¦¼ ì—†ìŒ
- ì¹´í…Œê³ ë¦¬:
  1) ì¸ì‚¬ë…¸ë¬´ ì¼ë°˜ ë‰´ìŠ¤ TOP 10
  2) ë…¸ë™ê´€ê³„ ë²•ë ¹ ê°œì • ë‰´ìŠ¤ TOP 10
  3) ê³ ìš©ë…¸ë™ë¶€ ë³´ë„ìë£Œ ë° ì •ì±… ì•Œë¦¼ TOP 5
  4) ê¸ˆìœµìœ„ì›íšŒ ë³´ë„ìë£Œ ë° ì •ì±… ì•Œë¦¼ TOP 5
  5) KCGS ê´€ë ¨ ë‰´ìŠ¤ TOP 1 (kcgs.or.kr ë°œí–‰ ì œì™¸)

í™˜ê²½ë³€ìˆ˜:
  TELEGRAM_TOKEN  : í…”ë ˆê·¸ë¨ ë´‡ í† í°
  TELEGRAM_CHAT_ID: ë³´ë‚´ì¤„ ì±„íŒ… ID
"""

import os
import re
import textwrap
from datetime import datetime
from urllib.parse import quote

import feedparser
import requests
from dateutil import parser as dateparser


############################
# í…”ë ˆê·¸ë¨
############################

def tg(msg: str) -> None:
    token = os.getenv("TELEGRAM_TOKEN", "")
    chat_id = os.getenv("TELEGRAM_CHAT_ID", "")
    if not token or not chat_id:
        return
    try:
        requests.post(
            f"https://api.telegram.org/bot{token}/sendMessage",
            json={"chat_id": chat_id, "text": msg, "parse_mode": "Markdown"},
            timeout=10,
        )
    except Exception:
        # í…”ë ˆê·¸ë¨ ì‹¤íŒ¨ëŠ” ì¡°ìš©íˆ ë¬´ì‹œ
        pass


############################
# ë‰´ìŠ¤ ìˆ˜ì§‘ ìœ í‹¸
############################

BASE_RSS = "https://news.google.com/rss/search?q={query}&hl=ko&gl=KR&ceid=KR:ko"
UA = "LaborNewsBot/1.0 (+https://github.com)"


def looks_korean(text: str) -> bool:
    """ì œëª©ì´ ê±°ì˜ ì˜ë¬¸ì´ë©´(ESG ì˜ë¬¸ ê¸°ì‚¬ ë“±) ë²„ë¦¬ê¸° ìœ„í•œ í•„í„°."""
    return bool(re.search(r"[ê°€-í£]", text or ""))


def make_url(query: str) -> str:
    return BASE_RSS.format(query=quote(query))


def fetch_feed(url: str):
    # feedparserê°€ ì§ì ‘ ê°€ì ¸ê°€ë„ ë˜ì§€ë§Œ timeout ë“±ì„ ìœ„í•´ requests ì‚¬ìš©
    try:
        resp = requests.get(url, headers={"User-Agent": UA}, timeout=15)
        resp.raise_for_status()
        return feedparser.parse(resp.text)
    except Exception:
        return feedparser.parse("")  # ë¹ˆ í”¼ë“œ


def normalize_entries(feed, limit: int, block_domains=None):
    block_domains = block_domains or []
    items = []
    for e in feed.entries:
        title = getattr(e, "title", "").strip()
        link = getattr(e, "link", "").strip()
        if not title or not link:
            continue
        # ë„ë©”ì¸ í•„í„°(KCGS ìì²´ ë‰´ìŠ¤ë¥¼ ì œì™¸í•  ë•Œ ì‚¬ìš©)
        if any(dom in link for dom in block_domains):
            continue
        if not looks_korean(title):
            # í•œê¸€ ê±°ì˜ ì—†ëŠ” ê¸°ì‚¬(ì˜ë¬¸ ESG ê¸°ì‚¬ ë“±) ì œê±°
            continue

        # ë‚ ì§œ íŒŒì‹±
        published = getattr(e, "published", "") or getattr(e, "updated", "")
        try:
            dt = dateparser.parse(published)
        except Exception:
            dt = None

        items.append(
            {
                "title": title,
                "link": link,
                "published": dt,
                "source": getattr(e, "source", getattr(e, "author", "")) or "",
            }
        )

    # ìµœì‹ ìˆœ ì •ë ¬
    items.sort(key=lambda x: x["published"] or datetime.min, reverse=True)
    # ì œëª© ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    seen = set()
    deduped = []
    for it in items:
        key = it["title"]
        if key in seen:
            continue
        seen.add(key)
        deduped.append(it)
        if len(deduped) >= limit:
            break
    return deduped


def format_items(title: str, items, max_items: int) -> str:
    if not items:
        return f"*{title}*\n- (í•´ë‹¹ ê¸°ê°„ì— ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤)\n\n"

    lines = [f"*{title}*"]
    for i, it in enumerate(items[:max_items], start=1):
        dt = it["published"]
        if dt:
            datestr = dt.strftime("%Y-%m-%d %H:%M")
        else:
            datestr = "ë‚ ì§œ ë¶ˆëª…"
        # í…”ë ˆê·¸ë¨ ë§ˆí¬ë‹¤ìš´ V2 íŠ¹ìˆ˜ë¬¸ì ê°„ë‹¨ ì´ìŠ¤ì¼€ì´í”„
        t = escape_md(it["title"])
        link = it["link"]
        lines.append(f"{i}. [{t}]({link})\n   - {datestr}")
    lines.append("")  # ë§ˆì§€ë§‰ì— ê°œí–‰
    return "\n".join(lines)


def escape_md(text: str) -> str:
    # í…”ë ˆê·¸ë¨ Markdownìš© ê°„ë‹¨ ì´ìŠ¤ì¼€ì´í”„
    return re.sub(r"([_*\[\]()~`>#+\-=|{}.!])", r"\\\1", text)


############################
# ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ ë¡œì§
############################

def get_category_1():
    """
    1) ë…¸ë™, ìœ¡ì•„, ì²­ë…„, ëª¨ì„±ë³´í˜¸, ì¶œì‚°, ì±„ìš©, íŒŒê²¬ê·¼ë¡œì, ì¥ì• ì¸, ê°€ì¡±ëŒë´„ ë“± ì¸ì‚¬ë…¸ë¬´ ë‰´ìŠ¤ TOP 10
    """
    query = (
        "(ë…¸ë™ OR ê·¼ë¡œ OR ì¸ì‚¬ë…¸ë¬´ OR HR OR ì¸ì‚¬íŒ€) "
        "AND (ìœ¡ì•„ OR ì²­ë…„ OR ëª¨ì„±ë³´í˜¸ OR ì¶œì‚° OR ì±„ìš© OR ëª¨ì§‘ OR íŒŒê²¬ê·¼ë¡œ OR "
        "ì¥ì• ì¸ ê³ ìš© OR ê°€ì¡±ëŒë´„ OR ì¼ê°€ì •ì–‘ë¦½)"
    )
    url = make_url(query)
    feed = fetch_feed(url)
    return normalize_entries(feed, limit=10)


def get_category_2():
    """
    2) ë…¸ë™ ê´€ê³„ ë²•ë ¹ ê°œì • ë‰´ìŠ¤ (ê·¼ë¡œê¸°ì¤€ë²•, ëª¨ì„±ë³´í˜¸, ë‚¨ë…€ê³ ìš©í‰ë“± ë“±) TOP 10
    """
    query = (
        "(ê·¼ë¡œê¸°ì¤€ë²• OR ë…¸ë™ê´€ê³„ë²• OR ë…¸ë™ë²• OR ë‚¨ë…€ê³ ìš©í‰ë“± OR ëª¨ì„±ë³´í˜¸ OR ìœ¡ì•„íœ´ì§ OR "
        "ì‚°ì—…ì•ˆì „ë³´ê±´ë²• OR íŒŒê²¬ê·¼ë¡œìë³´í˜¸ë²• OR ê¸°ê°„ì œë²• OR ê³ ìš©ì •ì±…ê¸°ë³¸ë²• OR ê·¼ë¡œìí‡´ì§ê¸‰ì—¬ë³´ì¥ë²•) "
        "AND (ê°œì • OR ê°œì •ì•ˆ OR ê°œí¸ OR ë²• ê°œì • OR ì‹œí–‰ë ¹ ê°œì • OR ì‹œí–‰ê·œì¹™ ê°œì • OR ì…ë²•ì˜ˆê³ )"
    )
    url = make_url(query)
    feed = fetch_feed(url)
    return normalize_entries(feed, limit=10)


def get_category_3():
    """
    3) ê³ ìš©ë…¸ë™ë¶€ ë³´ë„ìë£Œ ë° ì •ì±…ì•Œë¦¼ TOP 5
    - ë„ë©”ì¸: moel.go.kr
    """
    query = 'site:moel.go.kr ê³ ìš©ë…¸ë™ë¶€ ë³´ë„ìë£Œ OR ì •ì±…'
    url = make_url(query)
    feed = fetch_feed(url)
    return normalize_entries(feed, limit=5)


def get_category_4():
    """
    4) ê¸ˆìœµìœ„ì›íšŒ ë³´ë„ìë£Œ ë° ì •ì±…ì•Œë¦¼ TOP 5
    - ë„ë©”ì¸: fsc.go.kr
    """
    query = 'site:fsc.go.kr (ë³´ë„ìë£Œ OR ë³´ë„ ì°¸ê³ ìë£Œ OR ì •ì±…)'
    url = make_url(query)
    feed = fetch_feed(url)
    return normalize_entries(feed, limit=5)


def get_category_5():
    """
    5) KCGS(í•œêµ­ESGê¸°ì¤€ì›) ê´€ë ¨ ë‰´ìŠ¤ TOP 1
    - KCGSì—ì„œ ì§ì ‘ ë°œí–‰í•œ ë‰´ìŠ¤(kcgs.or.kr)ëŠ” ì œì™¸
    """
    query = '(KCGS OR "í•œêµ­ESGê¸°ì¤€ì›")'
    url = make_url(query)
    feed = fetch_feed(url)
    # kcgs.or.kr ë„ë©”ì¸ì€ ì œì™¸ (ë³¸ì¸ ë°œí–‰ì´ ì•„ë‹ˆë¼ "ê´€ë ¨ ë‰´ìŠ¤"ë§Œ í•„ìš”)
    return normalize_entries(feed, limit=1, block_domains=["kcgs.or.kr"])


############################
# ë©”ì¸
############################

def build_message() -> str:
    today = datetime.now().strftime("%Y-%m-%d")
    header = f"ğŸ”” ì¸ì‚¬ë…¸ë¬´Â·ë…¸ë™ë²•Â·ì •ì±… ë‰´ìŠ¤ ìš”ì•½ ({today})\n"
    header += "ESG ì¼ë°˜Â·ê¸ˆê°ì›(FSS) ê´€ë ¨ ì•Œë¦¼ì€ ì œì™¸, ìš”ì²­í•œ 5ê°œ ì¹´í…Œê³ ë¦¬ë§Œ í‘œì‹œí•©ë‹ˆë‹¤.\n\n"

    cat1 = format_items(
        "â‘  ë…¸ë™Â·ìœ¡ì•„Â·ì²­ë…„Â·ëª¨ì„±ë³´í˜¸Â·ì¶œì‚°Â·ì±„ìš©Â·íŒŒê²¬Â·ì¥ì• ì¸Â·ê°€ì¡±ëŒë´„ ë“± ì¸ì‚¬ë…¸ë¬´ ë‰´ìŠ¤ TOP 10",
        get_category_1(),
        10,
    )
    cat2 = format_items(
        "â‘¡ ë…¸ë™ ê´€ê³„ ë²•ë ¹ ê°œì • ê´€ë ¨ ë‰´ìŠ¤ TOP 10",
        get_category_2(),
        10,
    )
    cat3 = format_items(
        "â‘¢ ê³ ìš©ë…¸ë™ë¶€ ë³´ë„ìë£ŒÂ·ì •ì±… ì•Œë¦¼ TOP 5",
        get_category_3(),
        5,
    )
    cat4 = format_items(
        "â‘£ ê¸ˆìœµìœ„ì›íšŒ ë³´ë„ìë£ŒÂ·ì •ì±… ì•Œë¦¼ TOP 5",
        get_category_4(),
        5,
    )
    cat5 = format_items(
        "â‘¤ KCGS(í•œêµ­ESGê¸°ì¤€ì›) ê´€ë ¨ ë‰´ìŠ¤ TOP 1 (kcgs ì§ì ‘ ë°œí–‰ ì œì™¸)",
        get_category_5(),
        1,
    )

    msg = header + cat1 + cat2 + cat3 + cat4 + cat5
    # í…”ë ˆê·¸ë¨ 4096ì ì œí•œì— ëŒ€ë¹„í•´ ëŒ€ì¶© ì˜ë¼ë‘ê¸°
    if len(msg) > 4000:
        msg = msg[:3900] + "\n\n(ì´í•˜ ìƒëµë¨)"
    return msg


def main():
    msg = build_message()
    # í…”ë ˆê·¸ë¨ ì „ì†¡
    tg(msg)


if __name__ == "__main__":
    main()
